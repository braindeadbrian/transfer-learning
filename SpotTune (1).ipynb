{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsmM1pnJ_ooZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Source Dataset**"
      ],
      "metadata": {
        "id": "dR9tTOIfXKBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZdpeje9XPEq",
        "outputId": "95de886b-f3c3-4288-e11e-b50c5573124f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"a09\", \"a13\", \"a14\", \"a15\", \"a07\", \"a03\"]\n",
        "persons = [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\"]\n",
        "x_train_source = []\n",
        "y_train_source = []\n",
        "for cl in range(6):\n",
        "  for p in persons:\n",
        "    for s in range(60):\n",
        "      if s+1 < 10:\n",
        "        link = \"drive/My Drive/\"+classes[cl]+\"/\"+p+\"/s0\"+str(s+1)+\".txt\"\n",
        "      else:\n",
        "        link = \"drive/My Drive/\"+classes[cl]+\"/\"+p+\"/s\"+str(s+1)+\".txt\"\n",
        "      f = open(link, \"r\")\n",
        "      data_point = []\n",
        "      for x in f:\n",
        "        ax = [float(i) for i in x.split(',')[:6]]\n",
        "        data_point.append(ax)\n",
        "      x_train_source.append(data_point)\n",
        "      y_train_source.append(cl+1)"
      ],
      "metadata": {
        "id": "jJPiS48oZv_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_source = torch.from_numpy(np.array(x_train_source)).reshape(2880,125,6).type(torch.FloatTensor)\n",
        "y_train_source = torch.from_numpy(np.array(y_train_source)).type(torch.FloatTensor)"
      ],
      "metadata": {
        "id": "0ou_o3OlftrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target Dataset**"
      ],
      "metadata": {
        "id": "zWhmYSqaXMl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bax = torch.from_numpy(np.loadtxt(\"body_acc_x_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
        "bay = torch.from_numpy(np.loadtxt(\"body_acc_y_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
        "baz = torch.from_numpy(np.loadtxt(\"body_acc_z_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
        "bgx = torch.from_numpy(np.loadtxt(\"body_gyro_x_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
        "bgy = torch.from_numpy(np.loadtxt(\"body_gyro_y_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
        "bgz = torch.from_numpy(np.loadtxt(\"body_gyro_z_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
        "x_train = torch.cat((bax, bay, baz, bgx, bgy, bgz), 2)\n",
        "y_train = torch.from_numpy(np.loadtxt(\"y_train.txt\")).type(torch.FloatTensor) \n",
        "\n",
        "x_train = x_train[int(len(x_train)/2):]\n",
        "y_train = y_train[int(len(y_train)/2):]\n",
        "\n",
        "\n",
        "bax_test = torch.from_numpy(np.loadtxt(\"body_acc_x_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
        "bay_test = torch.from_numpy(np.loadtxt(\"body_acc_y_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
        "baz_test = torch.from_numpy(np.loadtxt(\"body_acc_z_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
        "bgx_test = torch.from_numpy(np.loadtxt(\"body_gyro_x_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
        "bgy_test = torch.from_numpy(np.loadtxt(\"body_gyro_y_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
        "bgz_test = torch.from_numpy(np.loadtxt(\"body_gyro_z_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
        "x_test = torch.cat((bax_test, bay_test, baz_test, bgx_test, bgy_test, bgz_test), 2)\n",
        "y_test = torch.from_numpy(np.loadtxt(\"y_test.txt\")).type(torch.FloatTensor) "
      ],
      "metadata": {
        "id": "VR7rVp0S5NJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, n_input, n_hidden,\n",
        "                 n_classes, drop_prob):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_classes = n_classes\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_input = n_input\n",
        "\n",
        "        self.lstm1 = nn.LSTM(n_input, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
        "        self.lstm3 = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
        "        #self.batch = nn.BatchNorm2d()\n",
        "        self.drop1 = nn.Dropout(drop_prob)\n",
        "        self.fc1 = nn.Linear(n_hidden, 1)\n",
        "        self.fc2 = nn.Linear(n_hidden, n_classes)\n",
        "        self.activation = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "        out, (hn,cn) = self.lstm1(x)\n",
        "        out, (hn,cn) = self.lstm2(out)\n",
        "        out, (hn,cn) = self.lstm3(out)\n",
        "        out = self.fc1(self.drop1(out))\n",
        "        #out = self.fc2(self.drop1(out[:,:,0]))\n",
        "        #out = self.fc1(out)\n",
        "        out = self.fc2(out[:,:,0])\n",
        "        out = self.activation(out)\n",
        "        return out #(torch.argmax(out, dim = 1) + 1)\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "        ''' Initialize hidden state'''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        # if (train_on_gpu):\n",
        "        if (torch.cuda.is_available() ):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "        return hidden\n"
      ],
      "metadata": {
        "id": "i6dM-QgAAHNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target):\n",
        "  preds = torch.argmax(output, dim = 1)\n",
        "  c = 0\n",
        "  for i in range(len(preds)):\n",
        "    if preds[i] == target[i]:\n",
        "      c += 1\n",
        "  return c/len(preds)"
      ],
      "metadata": {
        "id": "MQrexU_tOnz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train(model, X, Y, num_epochs, num_classes, batch_size):\n",
        "  criterion = nn.NLLLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
        "  #data = []\n",
        "  #for i in range(len(X)):\n",
        "  #  data.append([X[i], Y[i]])\n",
        "  dataset = torch.utils.data.TensorDataset(torch.Tensor(np.array(X)), torch.Tensor(np.array(Y)))\n",
        "  train_set_size = int(len(dataset)*0.8)\n",
        "  val_set_size = len(dataset) - train_set_size\n",
        "  train_set, val_set = torch.utils.data.random_split(dataset, [train_set_size, val_set_size])\n",
        "  trainloader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=100)\n",
        "  valloader = torch.utils.data.DataLoader(val_set, shuffle=True, batch_size=100)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    step_loss = 0\n",
        "    train_acc = []\n",
        "    model.train()\n",
        "    for step, (x, y) in enumerate(trainloader):\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model.forward(x.float())\n",
        "      loss = criterion(torch.log(outputs), y.long()-1)\n",
        "      train_acc.append(accuracy(outputs,y-1)*len(x))\n",
        "      #loss.requires_grad = True\n",
        "      loss.backward() #calculates the loss of the loss function\n",
        "      optimizer.step()\n",
        "      step_loss += loss\n",
        "    print(\"Training Loss: \",step_loss.item())\n",
        "    print(\"Training accuracy: \",np.sum(train_acc)/train_set_size)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = []\n",
        "    for step, (x, y) in enumerate(valloader):\n",
        "      outputs = model.forward(x.float())\n",
        "      loss = criterion(torch.log(outputs), y.long()-1)\n",
        "      val_acc.append(accuracy(outputs,y-1)*len(x))\n",
        "      #loss.requires_grad = True\n",
        "      val_loss += loss\n",
        "    print(\"Validation Loss: \",val_loss.item())\n",
        "    print(\"Validation accuracy: \",np.sum(val_acc)/val_set_size)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "-7sP4Z5ZMvRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#model = train(x_train_source, y_train_source, 20, 6, 100)\n",
        "pre_trained_model = LSTM(6,125,6,.3)\n",
        "pre_trained_model = train(pre_trained_model, x_train_source, y_train_source, 20, 6, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqSwlknl7XkS",
        "outputId": "9e209246-fdad-4f64-86a5-a39dbc13484e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  37.36975860595703\n",
            "Training accuracy:  0.30078125\n",
            "Validation Loss:  8.422944068908691\n",
            "Validation accuracy:  0.3385416666666667\n",
            "Training Loss:  32.06546401977539\n",
            "Training accuracy:  0.3424479166666667\n",
            "Validation Loss:  7.68480920791626\n",
            "Validation accuracy:  0.3784722222222222\n",
            "Training Loss:  31.327342987060547\n",
            "Training accuracy:  0.3793402777777778\n",
            "Validation Loss:  7.476946830749512\n",
            "Validation accuracy:  0.4270833333333333\n",
            "Training Loss:  29.910112380981445\n",
            "Training accuracy:  0.4210069444444444\n",
            "Validation Loss:  7.418369770050049\n",
            "Validation accuracy:  0.3854166666666667\n",
            "Training Loss:  28.981046676635742\n",
            "Training accuracy:  0.4288194444444444\n",
            "Validation Loss:  6.929805755615234\n",
            "Validation accuracy:  0.4826388888888889\n",
            "Training Loss:  25.13368797302246\n",
            "Training accuracy:  0.484375\n",
            "Validation Loss:  6.146280288696289\n",
            "Validation accuracy:  0.4947916666666667\n",
            "Training Loss:  22.57573890686035\n",
            "Training accuracy:  0.5720486111111112\n",
            "Validation Loss:  5.15699577331543\n",
            "Validation accuracy:  0.6006944444444444\n",
            "Training Loss:  20.83871841430664\n",
            "Training accuracy:  0.6011284722222222\n",
            "Validation Loss:  4.521377086639404\n",
            "Validation accuracy:  0.6371527777777778\n",
            "Training Loss:  16.999481201171875\n",
            "Training accuracy:  0.6675347222222222\n",
            "Validation Loss:  5.71044921875\n",
            "Validation accuracy:  0.5850694444444444\n",
            "Training Loss:  17.04015350341797\n",
            "Training accuracy:  0.6892361111111112\n",
            "Validation Loss:  4.067968368530273\n",
            "Validation accuracy:  0.6944444444444444\n",
            "Training Loss:  17.02924346923828\n",
            "Training accuracy:  0.7074652777777778\n",
            "Validation Loss:  3.762460708618164\n",
            "Validation accuracy:  0.6944444444444444\n",
            "Training Loss:  13.516522407531738\n",
            "Training accuracy:  0.7491319444444444\n",
            "Validation Loss:  3.2025816440582275\n",
            "Validation accuracy:  0.7552083333333334\n",
            "Training Loss:  12.01711654663086\n",
            "Training accuracy:  0.77734375\n",
            "Validation Loss:  2.6780152320861816\n",
            "Validation accuracy:  0.8350694444444444\n",
            "Training Loss:  11.858126640319824\n",
            "Training accuracy:  0.7821180555555556\n",
            "Validation Loss:  2.959437131881714\n",
            "Validation accuracy:  0.7673611111111112\n",
            "Training Loss:  11.672333717346191\n",
            "Training accuracy:  0.7890625\n",
            "Validation Loss:  2.544996976852417\n",
            "Validation accuracy:  0.8350694444444444\n",
            "Training Loss:  9.835260391235352\n",
            "Training accuracy:  0.8233506944444444\n",
            "Validation Loss:  2.0071659088134766\n",
            "Validation accuracy:  0.8663194444444444\n",
            "Training Loss:  7.536494255065918\n",
            "Training accuracy:  0.8637152777777778\n",
            "Validation Loss:  2.5309789180755615\n",
            "Validation accuracy:  0.7673611111111112\n",
            "Training Loss:  7.902583599090576\n",
            "Training accuracy:  0.8515625\n",
            "Validation Loss:  1.569882869720459\n",
            "Validation accuracy:  0.8784722222222222\n",
            "Training Loss:  6.914008140563965\n",
            "Training accuracy:  0.875\n",
            "Validation Loss:  1.6034103631973267\n",
            "Validation accuracy:  0.8802083333333334\n",
            "Training Loss:  6.48353385925293\n",
            "Training accuracy:  0.88671875\n",
            "Validation Loss:  2.003955125808716\n",
            "Validation accuracy:  0.8645833333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spottune"
      ],
      "metadata": {
        "id": "dn-3KzP6ifYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(nn.Module):\n",
        "  def __init__(self, n_input, n_hidden, n_output, drop_prob):\n",
        "    super(Agent, self).__init__()\n",
        "\n",
        "    self.n_input = n_input\n",
        "    self.n_output = n_output\n",
        "    self.n_hidden = n_hidden\n",
        "    self.drop_prob = drop_prob\n",
        "\n",
        "    self.lstm1 = nn.LSTM(n_input, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
        "    self.lstm2 = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
        "    #self.batch = nn.BatchNorm2d()\n",
        "    self.drop1 = nn.Dropout(drop_prob)\n",
        "    self.fc1 = nn.Linear(n_hidden, 1)\n",
        "    self.fc2 = nn.Linear(n_hidden, n_output)\n",
        "    self.activation = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "        out, (hn,cn) = self.lstm1(x)\n",
        "        out, (hn, cn) = self.lstm2(out)\n",
        "        out = self.fc1(self.drop1(out))\n",
        "        #out = self.fc2(self.drop1(out[:,:,0]))\n",
        "        #out = self.fc1(out)\n",
        "        out = self.fc2(out[:,:,0])\n",
        "        out = self.activation(out)\n",
        "        return out #(torch.argmax(out, dim = 1) + 1)\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "        ''' Initialize hidden state'''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        # if (train_on_gpu):\n",
        "        if (torch.cuda.is_available() ):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "        return hidden  "
      ],
      "metadata": {
        "id": "NMZx2TdgGhgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpottuneLSTM(nn.Module):\n",
        "  def __init__(self, n_input, n_hidden,\n",
        "                 n_classes, drop_prob):\n",
        "        super(SpottuneLSTM, self).__init__()\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_classes = n_classes\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_input = n_input\n",
        "\n",
        "        self.lstm1 = nn.LSTM(n_input, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
        "        self.lstm3 = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
        "\n",
        "        self.lstm1_frozen = nn.LSTM(n_input, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
        "        self.lstm2_frozen = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
        "        self.lstm3_frozen = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
        "\n",
        "\n",
        "        #self.batch = nn.BatchNorm2d()\n",
        "        self.drop1 = nn.Dropout(drop_prob)\n",
        "        self.fc1 = nn.Linear(n_hidden, 1)\n",
        "        self.fc2 = nn.Linear(n_hidden, n_classes)\n",
        "        self.activation = nn.Softmax(dim=1)\n",
        "  def lstm_output(self, lstm, lstm_frozen, policy, x, n_input):\n",
        "    for i in range(len(policy)):\n",
        "      ax = torch.reshape(x[i], (1, 125, n_input))\n",
        "      if policy[i] == 0.:\n",
        "        out_i, (_, _) = lstm_frozen(ax)\n",
        "      else:\n",
        "        out_i, (_, _) = lstm(ax)\n",
        "      if i == 0:\n",
        "        out = out_i\n",
        "      else:\n",
        "        out = torch.cat((out, out_i), 0)\n",
        "    return out\n",
        "  def forward(self, x, policy = None):\n",
        "        out = self.lstm_output(self.lstm1, self.lstm1_frozen, policy[:, 0], x, self.n_input)\n",
        "        #out, (hn,cn) = policy[:,0]*self.lstm1(x)+(1-policy[:,0])*self.lstm1_frozen(x)\n",
        "        #out, (hn,cn) = policy[:,1]*self.lstm2(x)+(1-policy[:,1])*self.lstm2_frozen(x)\n",
        "        out = self.lstm_output(self.lstm2, self.lstm2_frozen, policy[:, 1], out, self.n_hidden)\n",
        "        #out, (hn,cn) = policy[:,2]*self.lstm3(x)+(1-policy[:,2])*self.lstm3_frozen(x)\n",
        "        out = self.lstm_output(self.lstm3, self.lstm3_frozen, policy[:, 2], out, self.n_hidden)\n",
        "        out = self.fc1(self.drop1(out))\n",
        "        #out = self.fc2(self.drop1(out[:,:,0]))\n",
        "        #out = self.fc1(out)\n",
        "        out = self.fc2(out[:,:,0])\n",
        "        out = self.activation(out)\n",
        "        return out #(torch.argmax(out, dim = 1) + 1)"
      ],
      "metadata": {
        "id": "cXnuENJjigu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##It's working, it ust needed time and I wanted to go to sleep. Goodnight!!\n",
        "\n",
        "spottunemodel = SpottuneLSTM(6,125,6,.3)\n",
        "spottunemodel.lstm1_frozen.weight_ih_l0.requires_grad = False\n",
        "spottunemodel.lstm1_frozen.weight_hh_l0.requires_grad = False\n",
        "spottunemodel.lstm1_frozen.bias_ih_l0.requires_grad = False\n",
        "spottunemodel.lstm1_frozen.bias_hh_l0.requires_grad = False\n",
        "\n",
        "spottunemodel.lstm2_frozen.weight_ih_l0.requires_grad = False\n",
        "spottunemodel.lstm2_frozen.weight_hh_l0.requires_grad = False\n",
        "spottunemodel.lstm2_frozen.bias_ih_l0.requires_grad = False\n",
        "spottunemodel.lstm2_frozen.bias_hh_l0.requires_grad = False\n",
        "\n",
        "spottunemodel.lstm3_frozen.weight_ih_l0.requires_grad = False\n",
        "spottunemodel.lstm3_frozen.weight_hh_l0.requires_grad = False\n",
        "spottunemodel.lstm3_frozen.bias_ih_l0.requires_grad = False\n",
        "spottunemodel.lstm3_frozen.bias_hh_l0.requires_grad = False\n",
        "states = pre_trained_model.state_dict()\n",
        "states[\"lstm1_frozen.weight_ih_l0\"] = states[\"lstm1.weight_ih_l0\"]\n",
        "states[\"lstm1_frozen.weight_hh_l0\"] = states[\"lstm1.weight_hh_l0\"]\n",
        "states[\"lstm1_frozen.bias_ih_l0\"] = states[\"lstm1.bias_ih_l0\"]\n",
        "states[\"lstm1_frozen.bias_hh_l0\"] = states[\"lstm1.bias_hh_l0\"]\n",
        "states[\"lstm2_frozen.weight_ih_l0\"] = states[\"lstm2.weight_ih_l0\"]\n",
        "states[\"lstm2_frozen.weight_hh_l0\"] = states[\"lstm2.weight_hh_l0\"]\n",
        "states[\"lstm2_frozen.bias_ih_l0\"] = states[\"lstm2.bias_ih_l0\"]\n",
        "states[\"lstm2_frozen.bias_hh_l0\"] = states[\"lstm2.bias_hh_l0\"]\n",
        "states[\"lstm3_frozen.weight_ih_l0\"] = states[\"lstm3.weight_ih_l0\"]\n",
        "states[\"lstm3_frozen.weight_hh_l0\"] = states[\"lstm3.weight_hh_l0\"]\n",
        "states[\"lstm3_frozen.bias_ih_l0\"] = states[\"lstm3.bias_ih_l0\"]\n",
        "states[\"lstm3_frozen.bias_hh_l0\"] = states[\"lstm3.bias_hh_l0\"]\n",
        "spottunemodel.load_state_dict(states)\n",
        "\n",
        "def spottunetrain(model, agent, X, Y, num_epochs, num_classes, batch_size):\n",
        "  criterion = nn.NLLLoss()\n",
        "  optimizer_model = torch.optim.Adam(model.parameters(), lr=.001)\n",
        "  optimizer_agent = torch.optim.Adam(agent.parameters(), lr=.001)\n",
        "  dataset = torch.utils.data.TensorDataset(torch.Tensor(np.array(X)), torch.Tensor(np.array(Y)))\n",
        "  train_set_size = int(len(dataset)*0.8)\n",
        "  val_set_size = len(dataset) - train_set_size\n",
        "  train_set, val_set = torch.utils.data.random_split(dataset, [train_set_size, val_set_size])\n",
        "  trainloader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=100)\n",
        "  valloader = torch.utils.data.DataLoader(val_set, shuffle=True, batch_size=100)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    step_loss = 0\n",
        "    train_acc = []\n",
        "    for step, (x, y) in enumerate(trainloader):\n",
        "      optimizer_model.zero_grad()\n",
        "      optimizer_agent.zero_grad()\n",
        "      policy = agent(x)\n",
        "      action = F.gumbel_softmax(policy, tau=1, hard=True)\n",
        "      outputs = model.forward(x.float(), action)\n",
        "      loss = criterion(torch.log(outputs), y.long()-1)\n",
        "      train_acc.append(accuracy(outputs,y-1)*len(x))\n",
        "      #loss.requires_grad = True\n",
        "      loss.backward() #calculates the loss of the loss function\n",
        "      if action[0][0] == 0:\n",
        "        if spottunemodel.lstm1.weight_ih_l0.grad is not None:\n",
        "          spottunemodel.lstm1.weight_ih_l0.grad.data.zero_()\n",
        "        if spottunemodel.lstm1.weight_hh_l0.grad is not None:\n",
        "          spottunemodel.lstm1.weight_hh_l0.grad.data.zero_()\n",
        "        if spottunemodel.lstm1.bias_ih_l0.grad is not None:\n",
        "          spottunemodel.lstm1.bias_ih_l0.grad.data.zero_()\n",
        "        if spottunemodel.lstm1.bias_hh_l0.grad is not None:\n",
        "          spottunemodel.lstm1.bias_hh_l0.grad.data.zero_()\n",
        "      if action[0][1] == 0:\n",
        "        if spottunemodel.lstm2.weight_ih_l0.grad is not None:\n",
        "          spottunemodel.lstm2.weight_ih_l0.grad.data.zero_()\n",
        "        if spottunemodel.lstm2.weight_hh_l0.grad is not None:\n",
        "          spottunemodel.lstm2.weight_hh_l0.grad.data.zero_()\n",
        "        if spottunemodel.lstm2.bias_ih_l0.grad is not None:\n",
        "          spottunemodel.lstm2.bias_ih_l0.grad.data.zero_()\n",
        "        if spottunemodel.lstm2.bias_hh_l0.grad is not None:\n",
        "          spottunemodel.lstm2.bias_hh_l0.grad.data.zero_()\n",
        "      if action[0][2] == 0:\n",
        "        if spottunemodel.lstm3.weight_ih_l0.grad is not None:\n",
        "          spottunemodel.lstm3.weight_ih_l0.grad.data.zero_()\n",
        "        if spottunemodel.lstm3.weight_hh_l0.grad is not None:\n",
        "          spottunemodel.lstm3.weight_hh_l0.grad.data.zero_()\n",
        "        if spottunemodel.lstm3.bias_ih_l0.grad is not None:\n",
        "          spottunemodel.lstm3.bias_ih_l0.grad.data.zero_()\n",
        "        if spottunemodel.lstm3.bias_hh_l0.grad is not None:\n",
        "          spottunemodel.lstm3.bias_hh_l0.grad.data.zero_()\n",
        "      optimizer_model.step()\n",
        "      optimizer_agent.step()\n",
        "      step_loss += loss\n",
        "    print(\"Training Loss: \",step_loss.item())\n",
        "    print(\"Training Accuracy: \",np.sum(train_acc)/train_set_size)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = []\n",
        "    for step, (x, y) in enumerate(valloader):\n",
        "      policy = agent(x)\n",
        "      action = F.gumbel_softmax(policy, tau=1, hard=True)\n",
        "      outputs = model.forward(x.float(), action)\n",
        "      loss = criterion(torch.log(outputs), y.long()-1)\n",
        "      val_acc.append(accuracy(outputs,y-1)*len(x))\n",
        "      val_loss += loss\n",
        "    print(\"Validation Loss: \",val_loss.item())\n",
        "    print(\"Validation Accuracy: \",np.sum(val_acc)/val_set_size)\n",
        "  return model\n",
        "\n",
        "agent = Agent(6, 125, 3, 0.3)\n",
        "#spottunemodel = spottunetrain(spottunemodel, agent, x_train, y_train, 20, 6, 100)\n",
        "spottunemodel = spottunetrain(spottunemodel, agent, x_train, y_train, 5, 6, 100)"
      ],
      "metadata": {
        "id": "HnKtftR7io4G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "cc779b65-b88f-4fec-a46a-bf650a230571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6865b9af1e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##It's working, it ust needed time and I wanted to go to sleep. Goodnight!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspottunemodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpottuneLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mspottunemodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1_frozen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih_l0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspottunemodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1_frozen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh_l0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SpottuneLSTM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torch.utils.data.TensorDataset(torch.Tensor(np.array(x_test)), torch.Tensor(np.array(y_test)))\n",
        "testloader = torch.utils.data.DataLoader(testset, shuffle=True, batch_size=len(testset))\n",
        "criterion = nn.NLLLoss()\n",
        "spottunemodel.eval()\n",
        "test_loss = 0\n",
        "test_acc = []\n",
        "for step, (x, y) in enumerate(testloader):\n",
        "  policy = agent(x)\n",
        "  action = F.gumbel_softmax(policy, tau=1, hard=True)\n",
        "  outputs = spottunemodel.forward(x.float(), action)\n",
        "  loss = criterion(torch.log(outputs), y.long()-1)\n",
        "  test_acc.append(accuracy(outputs,y-1)*len(x))\n",
        "  test_loss += loss\n",
        "print(\"Validation Loss: \",test_loss.item())\n",
        "print(\"Validation Accuracy: \",np.sum(test_acc)/len(x_test))"
      ],
      "metadata": {
        "id": "Zy3GIoF78V0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benchmark**"
      ],
      "metadata": {
        "id": "1RhoUNsL4-Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_model = LSTM(6,125,6,.3)\n",
        "benchmark_model = train(benchmark_model, x_train, y_train, 20, 6, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "E_P80hpx49-T",
        "outputId": "19ae04dd-e118-449d-ce0e-1d9e9e05bc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  53.57286834716797\n",
            "Training accuracy:  0.18333333333333332\n",
            "Validation Loss:  14.223167419433594\n",
            "Validation accuracy:  0.20108695652173914\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b1f49e169948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbenchmark_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbenchmark_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmark_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-b81e032e6fe2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X, Y, num_epochs, num_classes, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-56e8fc259dc9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#out = self.fc2(self.drop1(out[:,:,0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 770\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torch.utils.data.TensorDataset(torch.Tensor(np.array(x_test)), torch.Tensor(np.array(y_test)))\n",
        "testloader = torch.utils.data.DataLoader(testset, shuffle=True, batch_size=len(testset))\n",
        "criterion = nn.NLLLoss()\n",
        "benchmark_model.eval()\n",
        "test_loss = 0\n",
        "test_acc = []\n",
        "for step, (x, y) in enumerate(testloader):\n",
        "  outputs = benchmark_model.forward(x.float())\n",
        "  loss = criterion(torch.log(outputs), y.long()-1)\n",
        "  test_acc.append(accuracy(outputs,y-1)*len(x))\n",
        "  test_loss += loss\n",
        "print(\"Test Loss: \",test_loss.item())\n",
        "print(\"Test Accuracy: \",np.sum(test_acc)/len(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUyv4gOR5h_E",
        "outputId": "8458d366-acb0-41ec-e5dc-5ad556915091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss:  1.095755934715271\n",
            "Test Accuracy:  0.45062775704105873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1\n",
        "Spottune:<br><br>\n",
        "Training Loss:  200.94908142089844<br>\n",
        "Training Accuracy:  0.1435374149659864<br>\n",
        "Validation Loss:  16.344161987304688<br>\n",
        "Validation Accuracy:  0.13722826086956522<br><br>\n",
        "Training Loss:  55.85991287231445<br>\n",
        "Training Accuracy:  0.18333333333333332<br>\n",
        "Validation Loss:  14.765766143798828<br>\n",
        "Validation Accuracy:  0.21603260869565216<br><br>\n",
        "Training Loss:  54.13177490234375<br>\n",
        "Training Accuracy:  0.2030612244897959<br>\n",
        "Validation Loss:  14.588086128234863<br>\n",
        "Validation Accuracy:  0.22010869565217392<br><br>\n",
        "Training Loss:  53.89824295043945<br>\n",
        "Training Accuracy:  0.20510204081632652<br>\n",
        "Validation Loss:  14.438674926757812<br>\n",
        "Validation Accuracy:  0.22146739130434784<br><br>\n",
        "Training Loss:  53.751136779785156<br>\n",
        "Training Accuracy:  0.21360544217687075<br>\n",
        "Validation Loss:  14.462971687316895<br>\n",
        "Validation Accuracy:  0.2105978260869565<br><br>\n",
        "Test Loss:  1.7950762510299683<br>\n",
        "Test Accuracy:  0.17712928401764505<br>\n",
        "<br>\n",
        "Regular LSTM:<br><br>\n",
        "Training Loss:  53.55483627319336<br>\n",
        "Training accuracy:  0.19795918367346937<br>\n",
        "Validation Loss:  14.216048240661621<br>\n",
        "Validation accuracy:  0.19293478260869565<br><br>\n",
        "Training Loss:  46.793758392333984<br>\n",
        "Training accuracy:  0.2693877551020408<br>\n",
        "Validation Loss:  9.893157005310059<br>\n",
        "Validation accuracy:  0.28940217391304346<br><br>\n",
        "Training Loss:  35.88084030151367<br>\n",
        "Training accuracy:  0.34251700680272107<br>\n",
        "Validation Loss:  9.668484687805176<br>\n",
        "Validation accuracy:  0.3383152173913043<br><br>\n",
        "Training Loss:  35.31169891357422<br>\n",
        "Training accuracy:  0.3476190476190476<br>\n",
        "Validation Loss:  9.428119659423828<br>\n",
        "Validation accuracy:  0.3451086956521739<br><br>\n",
        "Training Loss:  34.295257568359375<br>\n",
        "Training accuracy:  0.37755102040816324<br>\n",
        "Validation Loss:  9.395288467407227<br>\n",
        "Validation accuracy:  0.34375<br><br>\n",
        "Test Loss:  1.1956709623336792<br>\n",
        "Test Accuracy:  0.34781133355955207<br>"
      ],
      "metadata": {
        "id": "EFsjep6aw5eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2\n",
        "Spottune<br><br>\n",
        "Training Loss:  83.13389587402344<br>\n",
        "Training Accuracy:  0.1523809523809524<br>\n",
        "Validation Loss:  14.668261528015137<br>\n",
        "Validation Accuracy:  0.17798913043478262<br><br>\n",
        "Training Loss:  53.87263870239258<br>\n",
        "Training Accuracy:  0.19115646258503402<br>\n",
        "Validation Loss:  14.242119789123535<br>\n",
        "Validation Accuracy:  0.20244565217391305<br><br>\n",
        "Training Loss:  53.283512115478516<br>\n",
        "Training Accuracy:  0.204421768707483<br>\n",
        "Validation Loss:  14.249770164489746<br>\n",
        "Validation Accuracy:  0.19293478260869565<br><br>\n",
        "Training Loss:  53.08428192138672<br>\n",
        "Training Accuracy:  0.20204081632653062<br>\n",
        "Validation Loss:  14.192403793334961<br>\n",
        "Validation Accuracy:  0.1983695652173913<br><br>\n",
        "Training Loss:  53.044898986816406<br>\n",
        "Training Accuracy:  0.19421768707482992<br>\n",
        "Validation Loss:  14.245340347290039<br>\n",
        "Validation Accuracy:  0.2078804347826087<br>"
      ],
      "metadata": {
        "id": "Zr2nOHnsBCp3"
      }
    }
  ]
}